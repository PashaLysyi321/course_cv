{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Binary CLassifier\n",
    "\n",
    "In this notebook, we will implement a DNN classifier to classify the digits 0 and 1 from the MNIST dataset. The full classifier (for all digits) we will implement in the next lesson. The objective of this lesson is twofold:\n",
    "* To build our first DNN classifier (binary).\n",
    "* To demonstrate the importance of data normalization.\n",
    "\n",
    "Let's start with the ususal imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras import Model\n",
    "from time import time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "\n",
    "# Set the seeds for reproducibility\n",
    "from numpy.random import seed\n",
    "from tensorflow.random import set_seed\n",
    "seed_value = 1234578790\n",
    "seed(seed_value)\n",
    "set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Loading\n",
    "\n",
    "We have already inspected the MNIST dataset. We are going to load it now since we are going to use it for training the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Dataset params\n",
    "num_classes = 10\n",
    "size = x_train.shape[1]\n",
    "\n",
    "print('Train set:   ', len(y_train), 'samples')\n",
    "print('Test set:    ', len(y_test), 'samples')\n",
    "print('Sample dims: ', x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Preprocessing\n",
    "\n",
    "In this example, we are going to train a binary classifier to classify the digits 0 and 1. Therefore, we have to remove all other digits (classes) from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_train = np.logical_or(y_train == 0, y_train == 1)\n",
    "x_train = x_train[mask_train, ...]\n",
    "y_train = y_train[mask_train]\n",
    "\n",
    "mask_test = np.logical_or(y_test == 0, y_test == 1)\n",
    "x_test = x_test[mask_test, ...]\n",
    "y_test = y_test[mask_test]\n",
    "\n",
    "print('Train set:   ', len(y_train), 'samples')\n",
    "print('Test set:    ', len(y_test), 'samples')\n",
    "print('Sample dims: ', x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the Classifier\n",
    "\n",
    "We are going to build a relatively simple fully-connected DNN for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(size, size, 1))\n",
    "\n",
    "net = Flatten()(inputs)\n",
    "net = Dense(16, activation='relu')(net)  \n",
    "outputs = Dense(1, activation='linear')(net)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an extremely simple model (for a usualo classification task) yet it already contains several thousand of (trainable) parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "Let's now compile and train the model. We will use the well-known MSE as our loss function.\n",
    "\n",
    "Note: MSE is **not** the suitable loss for classification task but it serves us here well for the demonstration purposes. We will learn how to design a classifier in a proper way in the next lesson ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "batch_size = 128\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "start = time()\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "print('Elapsed time', time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot the history to see the evolution of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    h = history.history\n",
    "    epochs = range(len(h['loss']))\n",
    "\n",
    "    plt.subplot(121), plt.plot(epochs, h['loss'], '.-', epochs, h['val_loss'], '.-')\n",
    "    plt.grid(True), plt.xlabel('epochs'), plt.ylabel('loss')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "    plt.subplot(122), plt.plot(epochs, h['accuracy'], '.-',\n",
    "                               epochs, h['val_accuracy'], '.-')\n",
    "    plt.grid(True), plt.xlabel('epochs'), plt.ylabel('Accuracy')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "        \n",
    "    print('Train Acc     ', h['accuracy'][-1])\n",
    "    print('Validation Acc', h['val_accuracy'][-1])\n",
    "    \n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "From the history we see that the training performance is quite consistent with the validation (which is good, we will learn about the overfitting problem in next lessons). Now we are going to evaluate the trained classifier on the test dataset. Remember, this is the dataset that the network has **not** seen during the training and it will be used to assess the final performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "\n",
    "print('True', y_test[0:5].flatten())\n",
    "print('Pred', y_pred[0:5].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test.flatten()\n",
    "y_pred = y_pred.flatten() > 0.5\n",
    "\n",
    "# Overall accuracy\n",
    "num_samples = len(y_true)\n",
    "acc = np.sum(y_test == y_pred)/num_samples\n",
    "\n",
    "# Accuracy for digit 0\n",
    "mask = y_true == 0\n",
    "acc0 = np.sum(y_test[mask] == y_pred[mask])/np.sum(mask)\n",
    "\n",
    "# Accuracy for digit 1\n",
    "mask = y_true == 1\n",
    "acc1 = np.sum(y_test[mask] == y_pred[mask])/np.sum(mask)\n",
    "\n",
    "print('Overall acc', acc)\n",
    "print('Digit-0 acc', acc0)\n",
    "print('Digit-1 acc', acc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now visualise some of the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ii in range(15):\n",
    "    idx = np.random.randint(0, len(y_pred))\n",
    "    plt.subplot(3,5,ii+1), plt.imshow(x_test[idx, ...], cmap='gray')\n",
    "    plt.title('True: ' + str(y_true[idx]) + ' | Pred: ' + str(int(y_pred[idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization\n",
    "\n",
    "The dynamic range of the input signals (images) is [0, 255] and the (groundtruth) output lies within the interval [0, 1]. This is a huge disproportion between the input and output ranges and the network needs to learn to compensate for that. This will slow the convergence and, in general, yields poorer results. In order to reduce the negative effect of the input-output range mismatch, data **normalization** is necessary. The objective of data normalization is to harmonize the input and output ranges and to let the network to focus on learning the important classification features instead of learning also the range compensation factors. A more advanced concept is the data **standardisation** which we will cover later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data normalization\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-build the network to reinitilize the weights\n",
    "inputs = Input(shape=(size, size, 1))\n",
    "net = Flatten()(inputs)\n",
    "net = Dense(16, activation='relu')(net)  \n",
    "outputs = Dense(1, activation='linear')(net)\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Compile and train\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "start = time()\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=0)\n",
    "print('Elapsed time', time() - start)\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred = y_pred.flatten() > 0.5\n",
    "\n",
    "# Overall accuracy\n",
    "num_samples = len(y_true)\n",
    "acc = np.sum(y_test == y_pred)/num_samples\n",
    "\n",
    "# Accuracy for digit 0\n",
    "mask = y_true == 0\n",
    "acc0 = np.sum(y_test[mask] == y_pred[mask])/np.sum(mask)\n",
    "\n",
    "# Accuracy for digit 1\n",
    "mask = y_true == 1\n",
    "acc1 = np.sum(y_test[mask] == y_pred[mask])/np.sum(mask)\n",
    "\n",
    "print('Overall acc', acc)\n",
    "print('Digit-0 acc', acc0)\n",
    "print('Digit-1 acc', acc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "for ii in range(15):\n",
    "    idx = np.random.randint(0, len(y_pred))\n",
    "    plt.subplot(3,5,ii+1), plt.imshow(x_test[idx, ...], cmap='gray')\n",
    "    plt.title('True: ' + str(y_true[idx]) + ' | Pred: ' + str(int(y_pred[idx])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
