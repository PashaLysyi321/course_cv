{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Inspection\n",
    "\n",
    "When facing a ML problem, the very first thing you shall **always** do before starting your training endevours sit to inspect the dataset. In other words, look at the data, make some short analysis (and meta-analysis) to get a feeling about the type of data you are dealing with. We will demonstrate some basic yet useful techniques in this notebook.\n",
    "\n",
    "Throughout this notebook, we will be using the famous MNIST (Modified National Institute of Standards and Technology) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNIST (Dataset Definition)\n",
    "\n",
    "Before loading any data, you shall learn about the dataset itself, about its definition and structure. In our case, we will refer to the official documentation that you can find [here](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "There you learn that the dataset consists of 70 000 samples (60k for train and 10k for test). Each sample is a 28x28 monochrome image containing a handwritten digit. It follows that there are 10 different classes (one per each digit). In the provided link, you can also read about how the dataset has been built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Visualisation\n",
    "\n",
    "The next step is usually to look at the data themselves. In the beginning, load a subset of random data and visualize them. That way, you will get an impression of what type of data (in this case, what type of handwriting) we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST is one of the Keras built-in datasets and it can be easily loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Dataset params\n",
    "num_classes = 10\n",
    "size = x_train.shape[1]\n",
    "\n",
    "print('Train set:   ', len(y_train), 'samples')\n",
    "print('Test set:    ', len(y_test), 'samples')\n",
    "print('Sample dims: ', x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualise random subsets of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ii in range(15):\n",
    "    idx = np.random.randint(0, len(y_train))\n",
    "    plt.subplot(3,5,ii+1), plt.imshow(x_train[idx, ...], cmap='gray'), plt.title(y_train[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Histograms\n",
    "\n",
    "In order to correctly design you learning (training) strategies, it is important to analyse how well are the different classes represented in the dataset. In other words, we are trying to answer the following questions:\n",
    "* Do we have enough samples for each class?\n",
    "* Are there classes that are under-represented?\n",
    "* Are there classes that are over-represented?\n",
    "* Is the class distribution reflecting the real world?\n",
    "\n",
    "In order to answer these questions, a very powerful tool is to plot the class histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "centers = np.arange(0, num_classes + 1)\n",
    "counts, bounds = np.histogram(y_train, bins=centers-0.5)\n",
    "\n",
    "plt.bar(centers[:-1], counts), plt.grid(True)\n",
    "plt.xlabel('Class ID'), plt.ylabel('counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the class distribution is not totally homogeneous, we can conclude that no class is significantly over/under-represented. In relative terms, the histogram looks as shown below. A perfectly balanced dataset would have 10% of samples of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.bar(centers[:-1], counts/np.sum(counts)), plt.grid(True)\n",
    "plt.xlabel('Class ID'), plt.ylabel('counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation per Class\n",
    "\n",
    "Next, we are going to look at random different samples from the same class. The objective is to get a feeling about the amount of variability we can expect from samples corresponding to the same class (the so called inter-class variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Locate the samples for a given class ID\n",
    "class_id = 5\n",
    "mask = y_train == class_id\n",
    "\n",
    "# Extract the samples with the given class ID\n",
    "y_train_ = y_train[mask]\n",
    "x_train_ = x_train[mask, ...]\n",
    "\n",
    "# Plot random samples with class ID\n",
    "for ii in range(15):\n",
    "    idx = np.random.randint(0, len(y_train_))\n",
    "    plt.subplot(3,5,ii+1), plt.imshow(x_train_[idx, ...], cmap='gray'), plt.title(y_train_[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Number Templates\n",
    "\n",
    "We can observe that the samples do exhibit a certain amount of inter-class variance. However, can we say how a certain digit (out of the 10 possible) looks like on average? This is what we are going to do next, i.e., to plot the \"average digit\" for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for class_id in range(num_classes):\n",
    "    # Extract samples for the current class ID\n",
    "    mask = y_train == class_id\n",
    "    y_train_ = y_train[mask]\n",
    "    x_train_ = x_train[mask, ...]\n",
    "\n",
    "    # Compute the average representation\n",
    "    x_train_ = x_train_/255\n",
    "    avg = np.mean(x_train_, axis=0)\n",
    "    \n",
    "    # Show the result\n",
    "    plt.subplot(2,5,class_id+1), plt.imshow(avg/np.sum(avg), cmap='gray'), plt.title(class_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, one can also look on the (relative) average area that each digit occupy on the image. From the numbers below, conclude that the digit 1 is by far the smallest one (as expected) meanwhile the digits 0 (somehow suprisingly) and 8 (as expected) are the largest ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for class_id in range(num_classes):\n",
    "\n",
    "    mask = y_train == class_id\n",
    "    y_train_ = y_train[mask]\n",
    "    x_train_ = x_train[mask, ...]\n",
    "\n",
    "    x_train_ = x_train_/255\n",
    "    avg = np.mean(x_train_ > 0)\n",
    "\n",
    "    print('Avg rel area for', class_id, ':', avg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
