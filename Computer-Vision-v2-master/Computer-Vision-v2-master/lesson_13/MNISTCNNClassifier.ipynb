{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Classifier\n",
    "\n",
    "In this notebook, we will implement a CNN classifier to classify all the 10 digits the MNIST dataset. The objective of this lesson is twofold:\n",
    "* To build our first CNN classifier.\n",
    "* To use cross-entropy as classification loss.\n",
    "\n",
    "Let's start with the ususal imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Input, MaxPooling2D\n",
    "from tensorflow.keras import Model\n",
    "from time import time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "\n",
    "# Set the seeds for reproducibility\n",
    "from numpy.random import seed\n",
    "from tensorflow.random import set_seed\n",
    "seed_value = 1234578790\n",
    "seed(seed_value)\n",
    "set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Loading\n",
    "\n",
    "We have already inspected the MNIST dataset. We are going to load it now since we are going to use it for training the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Dataset params\n",
    "num_classes = 10\n",
    "size = x_train.shape[1]\n",
    "\n",
    "# Normalization\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "\n",
    "# One-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print('Train set:   ', len(y_train), 'samples')\n",
    "print('Test set:    ', len(y_test), 'samples')\n",
    "print('Sample dims: ', x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the Classifier\n",
    "\n",
    "We are going to build the same CNN classifier as before but note that we are using the softmax activation in the last layer (classification head)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(size, size, 1))\n",
    "\n",
    "net = Conv2D(16, kernel_size=(3, 3), activation=\"relu\")(inputs)\n",
    "net = MaxPooling2D(pool_size=(2, 2))(net)\n",
    "net = Conv2D(32, kernel_size=(3, 3), activation=\"relu\")(net)\n",
    "net = MaxPooling2D(pool_size=(2, 2))(net)\n",
    "net = Flatten()(net)\n",
    "outputs = Dense(num_classes, activation=\"softmax\")(net)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "Let's now compile and train the model. We will use the cross-entropy loss for this task. And note that we can use the built-in accuracy metric for monitoring the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "batch_size = 128\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "start = time()\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "print('Elapsed time', time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot the history to see the evolution of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    h = history.history\n",
    "    epochs = range(len(h['loss']))\n",
    "\n",
    "    plt.subplot(121), plt.plot(epochs, h['loss'], '.-', epochs, h['val_loss'], '.-')\n",
    "    plt.grid(True), plt.xlabel('epochs'), plt.ylabel('loss')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "    plt.subplot(122), plt.plot(epochs, h['accuracy'], '.-',\n",
    "                               epochs, h['val_accuracy'], '.-')\n",
    "    plt.grid(True), plt.xlabel('epochs'), plt.ylabel('Accuracy')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "        \n",
    "    print('Train Acc     ', h['accuracy'][-1])\n",
    "    print('Validation Acc', h['val_accuracy'][-1])\n",
    "    \n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have obtained an overall accuracy of about 99%. This might seem lower than we obtained using the binary classifier before but remember we are now classifying **all 10 digits**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.argmax(y_test, axis=1)\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "print('True', y_true[0:5])\n",
    "print('Pred', np.argmax(y_pred[0:5, :], axis=1))\n",
    "print('Pred', y_pred[0:5, :])\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = range(0, 10)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "for digit in digits:\n",
    "    mask = y_true == digit\n",
    "    \n",
    "    tp = np.sum(y_pred[mask] == digit)\n",
    "    total = np.sum(mask)\n",
    "    \n",
    "    print('Digit-', digit, ' acc', tp/total)\n",
    "    \n",
    "print('y_true', y_true[mask])\n",
    "print('y_pred', y_pred[mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained model can also be quickly evaluated by calling the evaluate method on the test data. It will return the loss and the metric (or metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test loss  ', ev[0])\n",
    "print('Test metric', ev[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now visualise some of the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ii in range(15):\n",
    "    idx = np.random.randint(0, len(y_pred))\n",
    "    plt.subplot(3,5,ii+1), plt.imshow(x_test[idx, ...], cmap='gray')\n",
    "    plt.title('True: ' + str(y_true[idx]) + ' | Pred: ' + str(y_pred[idx]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
