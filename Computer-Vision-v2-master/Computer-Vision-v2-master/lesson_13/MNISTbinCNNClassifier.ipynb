{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Binary Classifier\n",
    "\n",
    "In this notebook, we will implement a CNN classifier to classify the digits 0 and 1 from the MNIST dataset. The objective of this lesson is twofold:\n",
    "* To build our first CNN classifier (binary).\n",
    "* To demonstrate the importance of cross-entropy.\n",
    "* To compare the speed of CPU vs GPU training.\n",
    "\n",
    "Let's start with the ususal imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For comparison purposes, we will force Tensorflow to use CPU here\n",
    "# Cmment out this cell if you want to use GPU (if available)\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Input, MaxPooling2D\n",
    "from tensorflow.keras import Model\n",
    "from time import time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "\n",
    "# Set the seeds for reproducibility\n",
    "from numpy.random import seed\n",
    "from tensorflow.random import set_seed\n",
    "seed_value = 1234578790\n",
    "seed(seed_value)\n",
    "set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Loading\n",
    "\n",
    "We have already inspected the MNIST dataset. We are going to load it now since we are going to use it for training the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Dataset params\n",
    "num_classes = 10\n",
    "size = x_train.shape[1]\n",
    "\n",
    "print('Train set:   ', len(y_train), 'samples')\n",
    "print('Test set:    ', len(y_test), 'samples')\n",
    "print('Sample dims: ', x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Preprocessing\n",
    "\n",
    "In this example, we are going to train a binary classifier to classify the digits 0 and 1. Therefore, we have to remove all other digits (classes) from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_train = np.logical_or(y_train == 0, y_train == 1)\n",
    "x_train = x_train[mask_train, ...]\n",
    "y_train = y_train[mask_train]\n",
    "\n",
    "mask_test = np.logical_or(y_test == 0, y_test == 1)\n",
    "x_test = x_test[mask_test, ...]\n",
    "y_test = y_test[mask_test]\n",
    "\n",
    "# Normalization\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "\n",
    "print('Train set:   ', len(y_train), 'samples')\n",
    "print('Test set:    ', len(y_test), 'samples')\n",
    "print('Sample dims: ', x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the Classifier\n",
    "\n",
    "We are going to build a relatively simple convolutional neural network (CNN) for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(size, size, 1))\n",
    "\n",
    "net = Conv2D(16, kernel_size=(3, 3), activation=\"relu\")(inputs)\n",
    "net = MaxPooling2D(pool_size=(2, 2))(net)\n",
    "net = Conv2D(32, kernel_size=(3, 3), activation=\"relu\")(net)\n",
    "net = MaxPooling2D(pool_size=(2, 2))(net)\n",
    "net = Flatten()(net)\n",
    "outputs = Dense(1, activation=\"linear\")(net)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an extremely simple model (for a usual classification task) yet it already contains several thousand of (trainable) parameters. However, it still far less than the 12.5k parameters we used in the previous lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the model\n",
    "tf.keras.utils.plot_model(model, to_file=\"model.png\", show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "Let's now compile and train the model. We will use the well-known MSE as our loss function.\n",
    "\n",
    "Note: MSE is **not** the suitable loss for classification task but it serves us here well for the demonstration purposes. We will learn how to design a classifier in a proper way later in this lesson ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "batch_size = 128\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "start = time()\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "print('Elapsed time', time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot the history to see the evolution of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    h = history.history\n",
    "    epochs = range(len(h['loss']))\n",
    "\n",
    "    plt.subplot(121), plt.plot(epochs, h['loss'], '.-', epochs, h['val_loss'], '.-')\n",
    "    plt.grid(True), plt.xlabel('epochs'), plt.ylabel('loss')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "    plt.subplot(122), plt.plot(epochs, h['accuracy'], '.-',\n",
    "                               epochs, h['val_accuracy'], '.-')\n",
    "    plt.grid(True), plt.xlabel('epochs'), plt.ylabel('Accuracy')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "        \n",
    "    print('Train Acc     ', h['accuracy'][-1])\n",
    "    print('Validation Acc', h['val_accuracy'][-1])\n",
    "    \n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "As we did in the previous lesson, we are going to evaluate the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the predictions on the test set\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "print('True', y_test[0:5].flatten())\n",
    "print('Pred', y_pred[0:5].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test.flatten()\n",
    "y_pred = y_pred.flatten() > 0.5\n",
    "\n",
    "# Overall accuracy\n",
    "num_samples = len(y_true)\n",
    "acc = np.sum(y_test == y_pred)/num_samples\n",
    "\n",
    "# Accuracy for digit 0\n",
    "mask = y_true == 0\n",
    "acc0 = np.sum(y_test[mask] == y_pred[mask])/np.sum(mask)\n",
    "\n",
    "# Accuracy for digit 1\n",
    "mask = y_true == 1\n",
    "acc1 = np.sum(y_test[mask] == y_pred[mask])/np.sum(mask)\n",
    "\n",
    "print('Overall acc', acc)\n",
    "print('Digit-0 acc', acc0)\n",
    "print('Digit-1 acc', acc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now visualise some of the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ii in range(15):\n",
    "    idx = np.random.randint(0, len(y_pred))\n",
    "    plt.subplot(3,5,ii+1), plt.imshow(x_test[idx, ...], cmap='gray')\n",
    "    plt.title('True: ' + str(y_true[idx]) + ' | Pred: ' + str(int(y_pred[idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CPU vs GPU\n",
    "\n",
    "We are now going to repeat the process but training on the GPU. How did the training time changed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full MNIST Classification\n",
    "\n",
    "So far we have implemented a binary MNIST classifier that is able to classify handwritten zeros and ones. However, the MNIST dataset contains (obviously) more digits. How can we extend the classifier to account for all possible digits? Would a straightforward extension work? Let's see :-)\n",
    "\n",
    "First, we are going to load the data (and keep all the digits!) and build the exactly same CNN as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalization\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "\n",
    "print('Train set:   ', len(y_train), 'samples')\n",
    "print('Test set:    ', len(y_test), 'samples')\n",
    "print('Sample dims: ', x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(size, size, 1))\n",
    "\n",
    "net = Conv2D(16, kernel_size=(3, 3), activation=\"relu\")(inputs)\n",
    "net = MaxPooling2D(pool_size=(2, 2))(net)\n",
    "net = Conv2D(32, kernel_size=(3, 3), activation=\"relu\")(net)\n",
    "net = MaxPooling2D(pool_size=(2, 2))(net)\n",
    "net = Flatten()(net)\n",
    "outputs = Dense(1, activation=\"linear\")(net)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tranining\n",
    "\n",
    "We train the network the same way we did before. We are going to use the MSE as the loss function and demonstrate why it is not a good idea to use it for classification purposes :-) This is also the reason why we cannot use the built-in accuracy metric anymore.\n",
    "\n",
    "Also note that the training now takes longer since we are using all the available data (we didn't filter out any samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "batch_size = 128\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mse\"])\n",
    "\n",
    "start = time()\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "print('Elapsed time', time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "h = history.history\n",
    "epochs = range(len(h['loss']))\n",
    "\n",
    "plt.plot(epochs, h['loss'], '.-', epochs, h['val_loss'], '.-')\n",
    "plt.grid(True), plt.xlabel('epochs'), plt.ylabel('loss')\n",
    "plt.legend(['Train', 'Validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "No we are going to evaluate the classifier accuracy for all digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "\n",
    "print('True', y_test[100:105].flatten())\n",
    "print('Pred', y_pred[100:105].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_true = y_test.flatten()\n",
    "y_pred = y_pred.flatten()\n",
    "digits = range(0, 10)\n",
    "\n",
    "for digit in digits:\n",
    "    mask = y_true == digit    \n",
    "    # Count the true positives (the closest digit)\n",
    "    tp = np.sum(np.abs(y_pred[mask] - digit) < 0.5)\n",
    "    total = np.sum(mask)\n",
    "    print('Digit-', digit, ' acc', tp/total)\n",
    "    \n",
    "print('y_true', y_true[mask])\n",
    "print('y_pred', y_pred[mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the accuracy is actually quite bad and the predictions are not close enough to the ground truth. The reason is that we are using an inappropriate loss function for the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_true[mask], '.-')\n",
    "plt.plot(y_pred[mask], '.-')\n",
    "plt.grid(True)\n",
    "plt.legend(['y_true', 'y_pred'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
