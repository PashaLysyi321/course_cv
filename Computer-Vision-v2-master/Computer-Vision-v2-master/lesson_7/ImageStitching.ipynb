{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Stitching\n",
    "\n",
    "In this notebook we are going to demonstrate how feature detection and feature descriptors can help us to perform image stitching (in this case, to compose a panoramic view from 2 images). Let's start with the usual imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working with 2 images that capture the same scene but with **different camera position and different time**. The objective is to transfrom the source image to align with the target image. In other words, we want to find the homography that thrasnforms the source view into the target view and use this homography to stitch the two images together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load source view\n",
    "img_src = cv2.imread('data/image3_res.jpg')\n",
    "img_src = cv2.cvtColor(img_src, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Load target view\n",
    "img_dst = cv2.imread('data/image2_res.jpg')\n",
    "img_dst = cv2.cvtColor(img_dst, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.subplot(121), plt.imshow(img_src), plt.title('Source')\n",
    "plt.subplot(122), plt.imshow(img_dst), plt.title('Target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Detection\n",
    "\n",
    "In order to find the abovementioned homography, we need to find the pixel correspondences. To do so, we will be using the well-known SIFT features (you can use any detector + descriptor you want)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create the SIFT detector\n",
    "descriptor = cv2.SIFT_create()\n",
    "\n",
    "# We don't want to use any pixel mask, so we set it to None\n",
    "(kps_src, features_src) = descriptor.detectAndCompute(img_src, None)\n",
    "(kps_dst, features_dst) = descriptor.detectAndCompute(img_dst, None)\n",
    "\n",
    "# Let's draw the detected features\n",
    "feats_img_src = cv2.drawKeypoints(img_src, kps_src, None, color=(0,255,0))\n",
    "feats_img_dst = cv2.drawKeypoints(img_dst, kps_dst, None, color=(0,255,0))\n",
    "\n",
    "plt.subplot(121), plt.imshow(feats_img_src), plt.title('Source')\n",
    "plt.subplot(122), plt.imshow(feats_img_dst), plt.title('Target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see there is **a lot** of detected features and many of them are not relevant to our task (e.g. the waves on the river surface)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Matching\n",
    "\n",
    "Given the two sets of source and target features, we need to find the correspondences, i.e., for each feature on the source image we need to find the best matching feature on the target image. In this notebook, we will be using the **brute-force matcher with cross-check**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brute-force matcher with cross-check\n",
    "# Note: Euclidean norm, as the distance metric for feature descriptors, is suitable for SIFT,\n",
    "# but not necessarilly for other decriptors.\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "matches = bf.match(features_src, features_dst)\n",
    "    \n",
    "# Sort the features in order of distance (merely for plotting purposes)\n",
    "matches = sorted(matches, key = lambda x:x.distance)\n",
    "\n",
    "# Show how many features we had originally and how many matches we found\n",
    "print(\"Num features:\", len(features_src), len(features_dst))\n",
    "print(\"Num matches :\", len(matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the cross-check already took care and eliminated many of the irrelevant features (both on source and target side)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now draw the found matches (correspondences)\n",
    "result = cv2.drawMatches(img_src, kps_src, img_dst, kps_dst, matches,\n",
    "                         None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "plt.imshow(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now draw the best fitting matches (correspondences)\n",
    "result = cv2.drawMatches(img_src, kps_src, img_dst, kps_dst, matches[:100],\n",
    "                         None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "plt.imshow(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now draw the least fitting matches (correspondences)\n",
    "result = cv2.drawMatches(img_src, kps_src, img_dst, kps_dst, matches[-100:],\n",
    "                         None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "plt.imshow(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the correspondeces (i.e. the set of source points and the set of target points) we are going to find the best fitting homography transform. We will also switch the RANSAC on since this is a **powerful tool** to eliminate outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert keypoints to numpy arrays\n",
    "kps_src = np.float32([kp.pt for kp in kps_src])\n",
    "kps_dst = np.float32([kp.pt for kp in kps_dst])\n",
    "\n",
    "# Construct the two sets of points (source and target)\n",
    "pts_src = np.float32([kps_src[m.queryIdx] for m in matches])\n",
    "pts_dst = np.float32([kps_dst[m.trainIdx] for m in matches])\n",
    "\n",
    "# Estimate homography between source and target (try to see what happens if we switch RANSAC off)\n",
    "(H, status) = cv2.findHomography(pts_src, pts_dst, cv2.RANSAC, 4)\n",
    "\n",
    "print('Homography matrix:')\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Stitching\n",
    "\n",
    "Using the estimated homography, we are going to transform the source image to the target viewport and we will stitch the two images together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare canvas\n",
    "width = img_src.shape[1] + img_src.shape[1]\n",
    "height = img_src.shape[0] + img_src.shape[0]\n",
    "\n",
    "# Generate the stitched image\n",
    "result = cv2.warpPerspective(img_src, H, (width, height))\n",
    "result[0:img_dst.shape[0], 0:img_dst.shape[1]] = img_dst\n",
    "\n",
    "plt.imshow(result), plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the unnecessary black filling\n",
    "contour = np.sum(result, axis=-1)\n",
    "bottom = np.max(np.argmin(contour, axis=0))\n",
    "right = np.max(np.argmin(contour, axis=1))\n",
    "\n",
    "# Final result\n",
    "result = result[0:bottom, 0:right, :]\n",
    "plt.imshow(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the three images together for better comparison\n",
    "plt.subplot(131), plt.imshow(img_src), plt.title('Source')\n",
    "plt.subplot(132), plt.imshow(img_dst), plt.title('Target')\n",
    "plt.subplot(133), plt.imshow(result), plt.title('Panorama')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
