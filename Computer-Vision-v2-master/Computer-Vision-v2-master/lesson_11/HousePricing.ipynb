{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Neural Network Using Tensorflow Keras\n",
    "\n",
    "In this notebook we are going to implement our first neural network using Tensorflow (Keras). We will apply the network to the classic house pricing problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reproducibility reasons, we will set the seeds of the random number generators that are internally used throughout the notebook. This way we make sure that different runs of this notebook will always produce the same results. This is helpful if we want to evaluate the effect of a change that we have introduced, i.e., to compare the performance before and after the implementation of the change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "from tensorflow.random import set_seed\n",
    "seed_value = 1234578790\n",
    "seed(seed_value)\n",
    "set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Classic House Pricing Problem\n",
    "\n",
    "This classic task consists of estimating the price of a house (USA) based on the available description and attributes, i.e., the living area, number of bathrooms, construction year, etc. You can download the dataset from [here](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data).\n",
    "\n",
    "The dataset is stored in a comma separated file (csv). Let's load the dataset (the train split) and briefly explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/home/janko/Projects/robot_dreams/cv/data/datasets/house_pricing/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the loaded data. We'll see that each row corresponds to a sample (i.e., a particular house) and each column represents a certain attribute. There are 80 attributes in total (including the price). In this exercise, we will use a subset of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now have a quick look on the house prices. The `pandas` module already provides us with a handy function to do this :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset['SalePrice'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Our First Neural Network\n",
    "\n",
    "#### Data Preparation\n",
    "\n",
    "First, we are going to select the attributes we want to use for predicting the price. We will start very simple, with the living area (in square feet). Note that since not all samples (houses) have all of the attributes filled in, we will fill the missing values by the mean value of the dataset. If the dataset is complete, we don't have to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['SalePrice', 'GrLivArea']\n",
    "data = dataset[features]\n",
    "\n",
    "# Filling nan with the mean of the column\n",
    "data = data.fillna(data.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our inptus, i.e., the living areaa (in square feet) are usually in the order of 1e3. However, the outputs, i.e. the house prices, are usually in the order of 1e5. This is a huge mismatch in values. Even though the network will learn to cope with this, it is **always** a good idea to bring the inputs (all of them) and the outputs to the same order of magnitude, typically around 0 or 1. Therefore, we are going to **normalize** our data. We will learn more about data normalization in the next lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract input values (living area) and normalize\n",
    "# The dimensionality of the input data must always be explicit\n",
    "x = data[[features[1]]]\n",
    "mu, sigma = np.mean(x), np.std(x)\n",
    "x = (x - mu)/sigma\n",
    "\n",
    "# Extract output values (prices) and normalize\n",
    "y = data[features[0]].values\n",
    "y = y/100000\n",
    "\n",
    "# Split into 75% for train and 25% for test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the Network\n",
    "\n",
    "We are going to build our first neural network, a very simple one. For that, we will need to import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the model using the Sequential API from Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=x.shape[1], activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a model is built, it creates a computation graph. This graph needs to be compiled before we can start any trainings. In the compilation, we will tell Tensorflow what optimizer to use, what is the loss function and what metrics shall be used for monitoring the training. We can also define the list of callbacks to have further control over the training procedure but this is out of the scope of this lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer ='adam', loss = 'mean_squared_error', metrics =[metrics.mae])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally , let's visualize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple model with only one neuron. As we know, this neuron has two parameter: weight and bias.\n",
    "\n",
    "#### Training the Network\n",
    "\n",
    "Let's now train the network we have just built. In Keras this is super easy, it is a one line command!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, validation_data=(x_test,y_test), epochs=150, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better visualization, we are going to implement a function to show the training history and printing the training error as well as the absolute price prediction error (on the test split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    h = history.history\n",
    "    epochs = range(len(h['loss']))\n",
    "\n",
    "    plt.subplot(121), plt.plot(epochs, h['loss'], '.-', epochs, h['val_loss'], '.-')\n",
    "    plt.grid(True), plt.xlabel('epochs'), plt.ylabel('loss')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "    plt.subplot(122), plt.plot(epochs, np.array(h['mean_absolute_error'])*1e5, '.-',\n",
    "                               epochs, np.array(h['val_mean_absolute_error'])*1e5, '.-')\n",
    "    plt.grid(True), plt.xlabel('epochs'), plt.ylabel('MAE')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "        \n",
    "    print('Train MAE     ', h['mean_absolute_error'][-1]*1e5)\n",
    "    print('Validation MAE', h['val_mean_absolute_error'][-1]*1e5)\n",
    "    \n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the model makes an estimation error of almost 40 000 USD. That's quite a lot but have in mind that we have only used the living area as the parameter and a model of only one neuron. In fact, what does this model do? Let's print what the model has learnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now run the model to predict the value of a random house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 50\n",
    "pred = model.predict(x_test.iloc[[idx]])\n",
    "print(pred, y_test[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Improve the Model\n",
    "\n",
    "Given that the model is extremely simple, the performance is not that bad. But we shall be able to do better. Therefore, let's try to use more attributes and a bigger network.\n",
    "\n",
    "First, we are going to use (as inputs) more attributes (or features) for price estimation. Let's consider, for instance, the following ones:\n",
    "* **OverallQual**\n",
    " * Rates the overall material and finish of the house\n",
    "* **GrLivArea**\n",
    " * Above grade (ground) living area square feet\n",
    "* **GarageCars**\n",
    " * Size of garage in car capacity\n",
    "* **FullBath**\n",
    " * Full bathrooms above grade\n",
    "* **YearBuilt**\n",
    " * Original construction date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation\n",
    "\n",
    "As we did before, we are going to prepare the data but this time with all 5 attributes (features). For simplicity, we are going to use the `StandardScaler` from `sklearn`. But don't worry, it does exactly the same thing as we did before, it's just easier to use for multi-dimensional data :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['SalePrice','OverallQual', 'GrLivArea', 'GarageCars', 'FullBath', 'YearBuilt']\n",
    "data = dataset[features]\n",
    "\n",
    "# Filling nan with the mean of the column:\n",
    "data = data.fillna(data.mean())\n",
    "\n",
    "# Extract input values and normalize\n",
    "x = data[features[1:]]\n",
    "scale = StandardScaler()\n",
    "x = scale.fit_transform(x)\n",
    "\n",
    "# Extract output values (prices) and normalize\n",
    "y = data[features[0]].values\n",
    "y = y/100000\n",
    "\n",
    "# Split into 75% for train and 25% for test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the Network\n",
    "\n",
    "We are going to use the same 1-layer network. But remember, since we have now 5 features as inputs, the network (neuron in this case) makes a linear combination of all of them and adds the bias. Therefore, we have now 6 learnable (trainable) parameters, i.e. 5 weights plus bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=x.shape[1], activation='relu'))\n",
    "model.compile(optimizer ='adam', loss = 'mean_squared_error',  metrics =[metrics.mae])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Network\n",
    "\n",
    "We changed the data but the interface stays the same. Running the training is exactly the same as before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, validation_data=(x_test,y_test), epochs=150, batch_size=32, verbose=0)\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also see what the network has learnt about the feature relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features[1:])\n",
    "print(model.layers[0].get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Functional API\n",
    "\n",
    "Besides the Sequential API that we have been using so far, Keras also offers a different way to define the models. The Functional API is in general more verstile and it allows for branching (which the Sequential does not). In order to use the Functional API, we need to make two more imports (note that we don't need to import the Sequential anymore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Neural Network\n",
    "\n",
    "Let's now build our fisr deep neural network (DNN). Still a very simple one but consisting of 2 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=x.shape[1])\n",
    "outputs = Dense(5, activation='relu')(inputs)\n",
    "outputs = Dense(1, activation='linear')(outputs)\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer ='adam', loss ='mean_squared_error', metrics =[metrics.mae])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=150, batch_size=32, verbose=0)\n",
    "plot_history(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
