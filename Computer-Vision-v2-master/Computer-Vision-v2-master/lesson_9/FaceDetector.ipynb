{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Detection\n",
    "\n",
    "Face detection is a crucial step for many applications, especially for personal identification and access control. In this notebook, we are going to demonstrate the use of several tool, namely:\n",
    "* Viola-Jones object detector (applied to faces)\n",
    "* concept of facial landmarks\n",
    "* dlib library\n",
    "* face alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to load the Viola-Jones detector based on Haar cascades. The detector, already trained for detecting faces, is part of opencv contrib distribution and is located in its data subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casc_path = '/home/janko/Projects/envs/cv/lib/python3.8/site-packages/cv2/data/haarcascade_frontalface_default.xml'\n",
    "face_cascade = cv2.CascadeClassifier(casc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('/home/janko/Pictures/janko2.jpg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viola-Jones\n",
    "\n",
    "Viola-Jones is a classic and powerful algorithm for object detection. It is a sliding window approach that work in cascades and exploits Haar transform (basis functions) to learn object descriptors. It also makes uso of boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minNeighbors = 0 shows all the detection at all scale, a value of approx. 5 shall felter out all the spurious detections\n",
    "faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "print('Number of detected faces:', len(faces))\n",
    "\n",
    "# Draw rectangle around each face\n",
    "result = np.copy(img)\n",
    "faces_img = []\n",
    "for (x, y, w, h) in faces: \n",
    "    # Draw rectangle around the face\n",
    "    cv2.rectangle(result, (x, y), (x+w, y+h), (0, 255, 0), 3)\n",
    "    faces_img.append(img[y:y+h, x:x+w, :])\n",
    "    \n",
    "plt.imshow(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(121), plt.imshow(result, cmap='gray')\n",
    "plt.subplot(122), plt.imshow(faces_img[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Detection via dlib\n",
    "\n",
    "Dlib is a general purpose cross-platform software library that contains many useful tools. In particular, it includes a trained DNN for face detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "# Let's load the detector\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "# Detect faces, see http://dlib.net/face_detector.py.html\n",
    "# 1 --> upsampling factor\n",
    "rects = detector(gray, 1)\n",
    "\n",
    "print('Number of detected faces:', len(rects))\n",
    "print(rects)\n",
    "print(rects[0].left)\n",
    "\n",
    "def rect_to_bb(rect):\n",
    "    # Dlib rect --> OpenCV rect\n",
    "    x = rect.left()\n",
    "    y = rect.top()\n",
    "    w = rect.right() - x\n",
    "    h = rect.bottom() - y\n",
    "\n",
    "    return (x, y, w, h)\n",
    "\n",
    "\n",
    "# Draw rectangle around each face\n",
    "result_dlib = np.copy(img)\n",
    "faces_dlib_img = []\n",
    "for rect in rects:    \n",
    "    # Draw rectangle around the face\n",
    "    x, y, w, h = rect_to_bb(rect)\n",
    "    print(x, y, w, h)\n",
    "    cv2.rectangle(result_dlib, (x, y), (x+w, y+h), (0, 255, 0), 3)\n",
    "    faces_dlib_img.append(img[y:y+h, x:x+w, :])\n",
    "    \n",
    "\n",
    "plt.subplot(121), plt.imshow(result), plt.title('Viola-Jones')\n",
    "plt.subplot(122), plt.imshow(result_dlib), plt.title('dlib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facial Landmarks\n",
    "\n",
    "Facial landmarks can be considred as keypoints that define the contours of a face, eyes, nose and mouth. There are many models for facial landmarking, one of the most popular is the 68-point model (https://link.springer.com/article/10.1007/s11554-021-01107-w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the predictor (you need to download the predictor from an available source)\n",
    "predictor = dlib.shape_predictor('/media/janko/DATA/Datasets/shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "\n",
    "face = faces_dlib_img[1]\n",
    "rows, cols, _ = face.shape\n",
    "rect = dlib.rectangle(0, 0, cols, rows)\n",
    "shape = predictor(cv2.cvtColor(face, cv2.COLOR_RGB2GRAY), rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_landmarks(vec):\n",
    "    plt.plot(vec[0:17,0], vec[0:17,1], 'g.-')       # Contour\n",
    "    plt.plot(vec[48:68,0], vec[48:68,1], 'g.-')     # Mouth\n",
    "    plt.plot(vec[17:22,0], vec[17:22,1], 'g.-')     # Right eyebrow\n",
    "    plt.plot(vec[22:27,0], vec[22:27,1], 'g.-')     # Left eyebrow\n",
    "    plt.plot(np.concatenate((vec[36:42,0], vec[36:37,0])), np.concatenate((vec[36:42,1], vec[36:37,1])), 'g.-')     # Left eye\n",
    "    plt.plot(np.concatenate((vec[42:48,0], vec[42:43,0])), np.concatenate((vec[42:48,1], vec[42:43,1])), 'g.-')     # Right eye\n",
    "    plt.plot(vec[27:36,0], vec[27:36,1], 'g.-')     # Nose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert landmarks to ndarray for better manipulation\n",
    "kps = list(map(lambda p: (p.x, p.y), shape.parts()))\n",
    "landmarks = np.array(kps)\n",
    "print(kps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(face)\n",
    "plot_landmarks(landmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Alignment\n",
    "\n",
    "Face alignment is an important pre-processing step for face identification. The goal is to transform the detected faces as it was taken in frontal view. In general, only affine transforms are considered so the facial proportions are maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_template = np.load('data/face_template.npy')\n",
    "\n",
    "tpl_min, tpl_max = np.min(face_template, axis=0), np.max(face_template, axis=0)\n",
    "face_template = (face_template - tpl_min) / (tpl_max - tpl_min)\n",
    "\n",
    "plt.plot(face_template[:, 0], -face_template[:, 1], 'o')\n",
    "\n",
    "inner_triangle = [39, 42, 57]\n",
    "plt.plot(face_template[inner_triangle, 0], -face_template[inner_triangle, 1], 'rs')\n",
    "\n",
    "plt.axis('square')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_size = 256\n",
    "margin = 10\n",
    "out = np.copy(face)\n",
    "\n",
    "# Prepare landmarks\n",
    "landmarks = np.float32(landmarks)\n",
    "landmarks_idx = np.array(inner_triangle)\n",
    "\n",
    "# Adjust template (adjust to size, to margin and normalize back)\n",
    "template = face_template * out_size\n",
    "template = template + (margin/2)\n",
    "template = template / (out_size + margin)\n",
    "\n",
    "# Estimate affine transform\n",
    "H = cv2.getAffineTransform(landmarks[landmarks_idx], (out_size + margin) * template[landmarks_idx])\n",
    "\n",
    "# Rectify final image\n",
    "aligned = cv2.warpAffine(out, H, (out_size + margin, out_size + margin))\n",
    "\n",
    "plt.subplot(121), plt.imshow(face), plt.title('Detected')\n",
    "plt.subplot(122), plt.imshow(aligned), plt.title('Aligned')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
