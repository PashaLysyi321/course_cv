{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOMEWORK 14\n",
    "\n",
    "In this homework we will be working with the Fashion MNIST dataset. You will be given a classifier which suffers from considerable overfitting. Your objective will be to employ regularization techniques to mitigate the overfitting problem.\n",
    "\n",
    "Let's start with the usual imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten, Input, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras import Model\n",
    "from time import time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "\n",
    "# Set the seeds for reproducibility\n",
    "from numpy.random import seed\n",
    "from tensorflow.random import set_seed\n",
    "seed_value = 1234578790\n",
    "seed(seed_value)\n",
    "set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The MNIST fashgion dataset [link](https://github.com/zalandoresearch/fashion-mnist) was build by Zalando Reasearch tem consists of monochrome images of different type of clothing, namely:\n",
    "* 0\tT-shirt/top\n",
    "* 1\tTrouser\n",
    "* 2\tPullover\n",
    "* 3\tDress\n",
    "* 4\tCoat\n",
    "* 5\tSandal\n",
    "* 6\tShirt\n",
    "* 7\tSneaker\n",
    "* 8\tBag\n",
    "* 9\tAnkle boot\n",
    "\n",
    "It is also one of the Keras built-in datasets. Let's load the images and quickly inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Dataset params\n",
    "num_classes = 10\n",
    "size = x_train.shape[1]\n",
    "\n",
    "print('Train set:   ', len(y_train), 'samples')\n",
    "print('Test set:    ', len(y_test), 'samples')\n",
    "print('Sample dims: ', x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise some random samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnt = 1\n",
    "for r in range(3):\n",
    "    for c in range(6):\n",
    "        idx = np.random.randint(len(x_train))\n",
    "        plt.subplot(3,6,cnt)\n",
    "        plt.imshow(x_train[idx, ...], cmap='gray')\n",
    "        plt.title(y_train[idx])\n",
    "        cnt = cnt + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Classifier\n",
    "\n",
    "We are now going to build the baseline classifier that you will use throughout this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data normalization\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(28, 28, 1))\n",
    "net = Conv2D(32, kernel_size=(3, 3), activation=\"relu\", padding='same')(inputs)\n",
    "net = Flatten()(net)\n",
    "net = Dense(128)(net)\n",
    "outputs = Dense(10, activation=\"softmax\")(net)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 64\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    h = history.history\n",
    "    epochs = range(len(h['loss']))\n",
    "\n",
    "    plt.subplot(121), plt.plot(epochs, h['loss'], '.-', epochs, h['val_loss'], '.-')\n",
    "    plt.grid(True), plt.xlabel('epochs'), plt.ylabel('loss')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "    plt.subplot(122), plt.plot(epochs, h['accuracy'], '.-',\n",
    "                               epochs, h['val_accuracy'], '.-')\n",
    "    plt.grid(True), plt.xlabel('epochs'), plt.ylabel('Accuracy')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "        \n",
    "    print('Train Acc     ', h['accuracy'][-1])\n",
    "    print('Validation Acc', h['val_accuracy'][-1])\n",
    "    \n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the classifier suffers from massive overfitting. The validation accuracy is around 88% while the training accuracy is close to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combat the Overfitting!\n",
    "\n",
    "Now it is your turn. Use the classifier as a baseline, include some regularization techniques and try to improve the classification performance. You can try any techniques you might see fit, e.g.,\n",
    "* Dropout\n",
    "* Batch normalization\n",
    "* Weight regularization\n",
    "* Data augmentation\n",
    "* Early stopping\n",
    "* Pooling\n",
    "* Reducing the number of parameters (the size of the network)\n",
    "* ...\n",
    "\n",
    "There are to objective you shall fulfill in order to successfully complete this homework:\n",
    "* The validation accuracy shall be above 91%\n",
    "* Your network (with all the regularizations applied) shall **not** be larger than the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the baseline classifier in order to reduce the overfitting and make the performance more robust\n",
    "\n",
    "inputs = Input(shape=(28, 28, 1))\n",
    "net = Conv2D(32, kernel_size=(3, 3), activation=\"relu\", padding='same')(inputs)\n",
    "net = Flatten()(net)\n",
    "net = Dense(128)(net)\n",
    "outputs = Dense(10, activation=\"softmax\")(net)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "* What have you done in order to improve the performance?\n",
    "* Have you tried configurations that did not work out?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
